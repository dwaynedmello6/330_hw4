{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 4: Logistic regression, hyperparameter optimization \n",
    "### Associated lectures: [Lectures 7, 8](https://github.com/UBC-CS/cpsc330-2023W1/tree/main/lectures) \n",
    "\n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def my_sha1(text):\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points}\n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "- If you would like to use late tokens for the homework, all group members must have the necessary late tokens available. Please note that the late tokens will be counted for all members of the group.   \n",
    "\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W1/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "1. Before submitting the assignment, run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from \"1\" will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Follow the [CPSC 330 homework instructions](https://ubc-cs.github.io/cpsc330-2024W1/docs/homework_instructions.html), which include information on how to do your assignment and how to submit your assignment.\n",
    "4. Upload your solution on Gradescope. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. \n",
    "5. Make sure that the plots and output are rendered properly in your submitted file. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb so that the TAs can view your submission on Gradescope.\n",
    "\n",
    "\n",
    "_Note: The assignments will get gradually more open-ended as we progress through the course. In many cases, there won't be a single correct solution. Sometimes you will have to make your own choices and your own decisions (for example, on what parameter values to use when they are not explicitly provided in the instructions). Use your own judgment in such cases and justify your choices, if necessary._\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 1: implementing `DummyClassifier`\n",
    "<hr>\n",
    "rubric={autograde}\n",
    "\n",
    "In this course (unlike CPSC 340) you will generally **not** be asked to implement machine learning algorihtms (like logistic regression) from scratch. However, this exercise is an exception: you will implement the simplest possible classifier, `DummyClassifier`.\n",
    " \n",
    "As a reminder, `DummyClassifier` is meant as a baseline and is generally a simple \"model\" you could \"fit\" to a dataset. All it does is predict the most popular class in the training set. So if there are more 0s than 1s it predicts 0 every time, and if there are more 1s than 0s it predicts 1 every time. For `predict_proba` it looks at the frequencies in the training set, so if you have 30% 0's and 70% 1's it predicts `[0.3 0.7]` every time. Thus, `fit` only looks at `y` (not `X`).\n",
    "\n",
    "Below you will find starter code for a class called `MyDummyClassifier`, which has methods `fit()`, `predict()`, `predict_proba()` and `score()`. Your task is to fill in those four functions. To get you started, I have given you a `return` statement in each case that returns the correct data type: \n",
    "- `fit` returns nothing\n",
    "- `predict` returns an array whose size is the number of examples\n",
    "- `predict_proba` returns an array whose size is the number of examples x 2, and\n",
    "- `score` returns a float\n",
    "\n",
    "The next code block has some tests you can use to assess whether your code is working. \n",
    "\n",
    "I suggest starting with `fit` and `predict`, and making sure those are working before moving on to `predict_proba`. For `predict_proba`, you should return the frequency of each class in the training data. **Your `score` function should call your `predict` function**. Again, you can compare with `DummyClassifier` using the code below.\n",
    "\n",
    "`sklearn`'s `DummyClassifier` works when you have more than two classes, and also works if the target values are encoded differently, for example as \"cat\", \"dog\", \"eagle\", etc. However, for the sake of simplifying this question, we will consider **binary classification** only. Furthermore, we will assume that these classes are encoded as 0 and 1. In other words, you can safely assume that the variable y contains only 0's and 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "class MyDummyClassifier:\n",
    "    \"\"\"\n",
    "    A baseline classifier that predicts the most common class.\n",
    "    The predicted probabilities come from the relative frequencies\n",
    "    of the classes in the training data.\n",
    "\n",
    "    This implementation only works when y only contains 0's and 1's.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Dummy Classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Training data.\n",
    "        - y (array-like, shape (n_samples,)): Target labels (0's and 1's).\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # Replace with your code\n",
    "        self.most_common = mode(y).mode\n",
    "        self.most_common_prob = np.mean(y)\n",
    "        self.most_common_prob = 1 - self.most_common_prob\n",
    "\n",
    "        return None  # Replace with your code\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target labels for the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred (array-like, shape (n_samples,)): Predicted target labels.\n",
    "        \"\"\"\n",
    "        #predictions = np.zeros(X.shape[0])  # initializing with all predictions set to 0\n",
    "        # Replace with your code\n",
    "        predictions = np.full(X.shape[0], self.most_common) \n",
    "        return predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - probs (array-like, shape (n_samples, 2)): Predicted class probabilities.\n",
    "          Column 0 corresponds to class 0, and column 1 corresponds to class 1.\n",
    "        \"\"\"\n",
    "        probs = np.zeros((X.shape[0], 2))  # initializing all probabilities set to 0.\n",
    "        # Replace with your code\n",
    "        probs[:, 1] = self.most_common_prob\n",
    "        probs[:, 0] = 1 - self.most_common_prob\n",
    "\n",
    "        return probs  # Replace with your code\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model on the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Input data.\n",
    "        - y (array-like, shape (n_samples,)): True target labels.\n",
    "\n",
    "        Returns:\n",
    "        - accuracy (float): Accuracy of the model on the input data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Replace with your code\n",
    "        predictions = self.predict(X)\n",
    "        c = 0\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == y[i]:\n",
    "                c += 1\n",
    "\n",
    "        accuracy = c / len(predictions)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Below are some tests for `predict` using randomly generated data. You may want to run the cell a few times to make sure you explore the different cases (or automate this with a loop or random seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# For testing, generate random data\n",
    "n_train = 101\n",
    "n_valid = 21\n",
    "d = 5\n",
    "X_train_dummy = np.random.randn(n_train, d)\n",
    "X_valid_dummy = np.random.randn(n_valid, d)\n",
    "y_train_dummy = np.random.randint(2, size=n_train)\n",
    "y_valid_dummy = np.random.randint(2, size=n_valid)\n",
    "\n",
    "my_dc = MyDummyClassifier()\n",
    "sk_dc = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "my_dc.fit(X_train_dummy, y_train_dummy)\n",
    "sk_dc.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "assert np.array_equal(my_dc.predict(X_train_dummy), sk_dc.predict(X_train_dummy))\n",
    "assert np.array_equal(my_dc.predict(X_valid_dummy), sk_dc.predict(X_valid_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Below are some tests for `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[1;32m      2\u001b[0m     my_dc\u001b[38;5;241m.\u001b[39mpredict_proba(X_train_dummy), sk_dc\u001b[38;5;241m.\u001b[39mpredict_proba(X_train_dummy)\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[1;32m      5\u001b[0m     my_dc\u001b[38;5;241m.\u001b[39mpredict_proba(X_valid_dummy), sk_dc\u001b[38;5;241m.\u001b[39mpredict_proba(X_valid_dummy)\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert np.allclose(\n",
    "    my_dc.predict_proba(X_train_dummy), sk_dc.predict_proba(X_train_dummy)\n",
    ")\n",
    "assert np.allclose(\n",
    "    my_dc.predict_proba(X_valid_dummy), sk_dc.predict_proba(X_valid_dummy)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Below are some tests for `score`.\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert np.isclose(\n",
    "    my_dc.score(X_train_dummy, y_train_dummy), sk_dc.score(X_train_dummy, y_train_dummy)\n",
    ")\n",
    "assert np.isclose(\n",
    "    my_dc.score(X_valid_dummy, y_valid_dummy), sk_dc.score(X_valid_dummy, y_valid_dummy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed! üçÄ</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e3cc53df86a7e14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercise 2: Trump Tweets\n",
    "<hr>\n",
    "\n",
    "For the rest of this assignment we'll be working with a [dataset of Donald Trump's tweets](https://www.kaggle.com/austinreese/trump-tweets) as of June 2020. You should start by downloading the dataset. Unzip it and move the file `realdonaldtrump.csv` under the data directory in this folder. As usual, please do not submit the dataset when you submit the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1698308935</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701461182</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737479987</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741160716</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773561338</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         link  \\\n",
       "id                                                              \n",
       "1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                                      content  \\\n",
       "id                                                              \n",
       "1698308935  Be sure to tune in and watch Donald Trump on L...   \n",
       "1701461182  Donald Trump will be appearing on The View tom...   \n",
       "1737479987  Donald Trump reads Top Ten Financial Tips on L...   \n",
       "1741160716  New Blog Post: Celebrity Apprentice Finale and...   \n",
       "1773561338  \"My persona will never be that of a wallflower...   \n",
       "\n",
       "                           date  retweets  favorites mentions hashtags  \n",
       "id                                                                      \n",
       "1698308935  2009-05-04 13:54:25       510        917      NaN      NaN  \n",
       "1701461182  2009-05-04 20:00:10        34        267      NaN      NaN  \n",
       "1737479987  2009-05-08 08:38:08        13         19      NaN      NaN  \n",
       "1741160716  2009-05-08 15:40:15        11         26      NaN      NaN  \n",
       "1773561338  2009-05-12 09:07:28      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv(\"data/realdonaldtrump.csv\", index_col=0)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43352, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to predict whether a tweet will go \"viral\", defined as having more than 10,000 retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets_df[\"retweets\"] > 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, we'll be using only the content (text) of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_df[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this assignment, you can ignore all the other columns in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Ordering the steps\n",
    "rubric={points}\n",
    "\n",
    "Let's start by building a model using `CountVectorizer` and `LogisticRegression`. The code required to do this has been provided below, but in the wrong order. \n",
    "\n",
    "**Your tasks:**\n",
    "1. Rearrange the lines of code to correctly fit the model and compute the cross-validation score. \n",
    "2. Add a short comment above each step to describe what the code is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR COMMENT HERE\n",
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.6, random_state=123\n",
    ")\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "cv_score = cross_val_score(pipe, X_train, y_train).mean()\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "pipe = make_pipeline(countvec, lr)\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "lr = LogisticRegression(max_iter=1000, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8953863898500576)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First split the data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.6, random_state=123\n",
    ")\n",
    "\n",
    "# Creating the count vectorizer object\n",
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# creating the LogisticRegression model\n",
    "lr = LogisticRegression(max_iter=1000, random_state=123)\n",
    "\n",
    "# creating a pipeling with both the data processing and the model to avoid breaking the Golden Rule\n",
    "pipe = make_pipeline(countvec, lr)\n",
    "\n",
    "# Cross validating the model with train data to get the mean\n",
    "cv_score = cross_val_score(pipe, X_train, y_train).mean()\n",
    "cv_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.2 Baseline\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Run 5-fold cross-validation with `sklearn`'s `DummyClassifier` on this dataset. Store the mean cross-validation score in a variable called `dummy_cv_score` given below.\n",
    "\n",
    "> You should be able to use `MyDummyClassifier` above. But sklearn will likely complain if you try to pass it to `cross_val_score` or `cross_validate`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7423875432525952)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_cls = DummyClassifier(strategy = 'most_frequent')\n",
    "dummy_cv_score = cross_val_score(dummy_cls, X_train, y_train, cv=5).mean()\n",
    "dummy_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.2</pre></strong> passed! üåà</p>"
      ],
      "text/plain": [
       "q2.2 results: All test cases passed!"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Discussion\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Compare the `DummyClassifier` score to what you got from logistic regression above. Does logistic regression seem to be doing anything useful?\n",
    "2. Above we averaged the scores from the 5 folds of cross-validation of logistic regression. Print out the 5 individual scores. Reminder: `sklearn` calls them `\"test_score\"` but they are really (cross-)validation scores. \n",
    "3. Are the 5 scores close to each other or spread far apart? (This is a bit subjective, answer to the best of your ability.) \n",
    "4. How does the size of this dataset (number of rows) compare to [the cities dataset](https://github.com/UBC-CS/cpsc330-2023W1/blob/main/lectures/data/canada_usa_cities.csv) we have been using in the lecture notes? How does this relate to the different sub-scores from the 5 folds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The logistic regression model has a noticeably higher average cross-validation score compared to the dummy classifier, at 89% versus 74%, indicating that it's indeed effective.\n",
    "2. Finished below\n",
    "3. In my view, the five scores are very similar, with a difference of only 0.09% between them, so I would consider them to be quite close.\n",
    "4. It is significantly larger than that dataset, which indicates that the scores are more consistent and closely aligned with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89186851, 0.90023068, 0.89561707, 0.88956171, 0.89965398])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores = cross_val_score(pipe, X_train, y_train)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba1f8ea22638cf75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.4 Probability scores\n",
    "rubric={autograde}\n",
    "\n",
    "Here we train a logistic regression classifier on the entire training set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    CountVectorizer(stop_words=\"english\"),\n",
    "    LogisticRegression(max_iter=1000, random_state=123),\n",
    ")\n",
    "pipe_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Your tasks:**\n",
    "\n",
    "1. Using this model, find the tweet in the **test set** with the highest predicted probability of being viral. Store the tweet and the associated probability in the variables `tweet` and `prob`, respectively. \n",
    "\n",
    "> Reminder: you are free to reuse/adapt code from lecture. Please add in a small attribution, e.g. \"From Lecture 7\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss_prob = pipe_lr.predict_proba(X_test)[:, 1]\n",
    "most_effective = np.argmax(poss_prob)\n",
    "\n",
    "tweet = X_test.iloc[most_effective]\n",
    "prob = poss_prob[most_effective]\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.4</pre></strong> passed! üåü</p>"
      ],
      "text/plain": [
       "q2.4 results: All test cases passed!"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f910e9d1d6d09182",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.5 Coefficients\n",
    "rubric={autograde}\n",
    "\n",
    "We can extract the `CountVectorizer` and `LogisticRegression` objects from the `Pipeline` object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "vec_from_pipe = pipe_lr.named_steps[\"countvectorizer\"]\n",
    "lr_from_pipe = pipe_lr.named_steps[\"logisticregression\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Your tasks:**\n",
    "\n",
    "Using these extracted components above, get the five words with the highest coefficients and 5 words with smallest coefficients. Store them as lists in `top_5_words` and `bottom_5_words` variables, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(vec_from_pipe.get_feature_names_out())\n",
    "coefficients = lr_from_pipe.coef_[0].flatten()\n",
    "\n",
    "top_5_index = np.argsort(coefficients)[-5:]\n",
    "bottom_5_index = np.argsort(coefficients)[:5]\n",
    "\n",
    "top_5_words = feature_names[top_5_index]  # Store them as a list\n",
    "bottom_5_words = feature_names[bottom_5_index]  # Store them as a list\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fake', 'coronavirus', 'harassment', 'transcripts', 'democrats'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['realdonaldtrump', 'barackobama', 'trump2016pic', 'donaldtrump',\n",
       "       'thanks'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong style='color: red;'><pre style='display: inline;'>q2.5</pre> results:</strong></p><p><strong><pre style='display: inline;'>q2.5 - 1</pre> result:</strong></p><pre>    ‚ùå Test case failed\n",
       "    Trying:\n",
       "        assert not top_5_words is None, 'Are you using the correct variable?'\n",
       "    Expecting nothing\n",
       "    ok\n",
       "    Trying:\n",
       "        assert not bottom_5_words is None, 'Are you using the correct variable?'\n",
       "    Expecting nothing\n",
       "    ok\n",
       "    Trying:\n",
       "        assert len(top_5_words) == 5, 'Are you getting the top 5 words?'\n",
       "    Expecting nothing\n",
       "    ok\n",
       "    Trying:\n",
       "        assert len(bottom_5_words) == 5, 'Are you getting the bottom 5 words?'\n",
       "    Expecting nothing\n",
       "    ok\n",
       "    Trying:\n",
       "        assert isinstance(top_5_words, list), 'Are you storing the top 5 words in a list?'\n",
       "    Expecting nothing\n",
       "    **********************************************************************\n",
       "    Line 5, in q2.5 0\n",
       "    Failed example:\n",
       "        assert isinstance(top_5_words, list), 'Are you storing the top 5 words in a list?'\n",
       "    Exception raised:\n",
       "        Traceback (most recent call last):\n",
       "          File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/doctest.py\", line 1336, in __run\n",
       "            exec(compile(example.source, filename, \"single\",\n",
       "          File \"<doctest q2.5 0[4]>\", line 1, in <module>\n",
       "            assert isinstance(top_5_words, list), 'Are you storing the top 5 words in a list?'\n",
       "        AssertionError: Are you storing the top 5 words in a list?\n",
       "    Trying:\n",
       "        assert isinstance(bottom_5_words, list), 'Are you storing the bottom 5 words in a list?'\n",
       "    Expecting nothing\n",
       "    **********************************************************************\n",
       "    Line 6, in q2.5 0\n",
       "    Failed example:\n",
       "        assert isinstance(bottom_5_words, list), 'Are you storing the bottom 5 words in a list?'\n",
       "    Exception raised:\n",
       "        Traceback (most recent call last):\n",
       "          File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/doctest.py\", line 1336, in __run\n",
       "            exec(compile(example.source, filename, \"single\",\n",
       "          File \"<doctest q2.5 0[5]>\", line 1, in <module>\n",
       "            assert isinstance(bottom_5_words, list), 'Are you storing the bottom 5 words in a list?'\n",
       "        AssertionError: Are you storing the bottom 5 words in a list?\n",
       "    Trying:\n",
       "        assert my_sha1(''.join(sorted(top_5_words))) == 'd57c4319aa6e91c93c59a8c2ed01401cbad7e072', 'Incorrect words in `top_5_words`.'\n",
       "    Expecting nothing\n",
       "    ok\n",
       "    Trying:\n",
       "        assert my_sha1(''.join(sorted(bottom_5_words))) == '5e2af1c788307cca183334510cf6bbee0208cab2', 'Incorrect words in `bottom_5_words`.'\n",
       "    Expecting nothing\n",
       "    ok\n",
       "</pre>"
      ],
      "text/plain": [
       "q2.5 results:\n",
       "    q2.5 - 1 result:\n",
       "        ‚ùå Test case failed\n",
       "        Trying:\n",
       "            assert not top_5_words is None, 'Are you using the correct variable?'\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            assert not bottom_5_words is None, 'Are you using the correct variable?'\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            assert len(top_5_words) == 5, 'Are you getting the top 5 words?'\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            assert len(bottom_5_words) == 5, 'Are you getting the bottom 5 words?'\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            assert isinstance(top_5_words, list), 'Are you storing the top 5 words in a list?'\n",
       "        Expecting nothing\n",
       "        **********************************************************************\n",
       "        Line 5, in q2.5 0\n",
       "        Failed example:\n",
       "            assert isinstance(top_5_words, list), 'Are you storing the top 5 words in a list?'\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/doctest.py\", line 1336, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q2.5 0[4]>\", line 1, in <module>\n",
       "                assert isinstance(top_5_words, list), 'Are you storing the top 5 words in a list?'\n",
       "            AssertionError: Are you storing the top 5 words in a list?\n",
       "        Trying:\n",
       "            assert isinstance(bottom_5_words, list), 'Are you storing the bottom 5 words in a list?'\n",
       "        Expecting nothing\n",
       "        **********************************************************************\n",
       "        Line 6, in q2.5 0\n",
       "        Failed example:\n",
       "            assert isinstance(bottom_5_words, list), 'Are you storing the bottom 5 words in a list?'\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/doctest.py\", line 1336, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q2.5 0[5]>\", line 1, in <module>\n",
       "                assert isinstance(bottom_5_words, list), 'Are you storing the bottom 5 words in a list?'\n",
       "            AssertionError: Are you storing the bottom 5 words in a list?\n",
       "        Trying:\n",
       "            assert my_sha1(''.join(sorted(top_5_words))) == 'd57c4319aa6e91c93c59a8c2ed01401cbad7e072', 'Incorrect words in `top_5_words`.'\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            assert my_sha1(''.join(sorted(bottom_5_words))) == '5e2af1c788307cca183334510cf6bbee0208cab2', 'Incorrect words in `bottom_5_words`.'\n",
       "        Expecting nothing\n",
       "        ok"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.6 Running a cross-validation fold without sklearn tools \n",
    "rubric={autograde}\n",
    "\n",
    "Sklearn provides a lot of useful tools like `make_pipeline` and `cross_validate`, which are awesome. But with these fancy tools it's also easy to lose track of what is actually happening under the hood. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Compute logistic regression's validation score on the first fold, that is, train on 80% and validate on 20% of the training data (`X_train`) without using sklearn `Pipeline` or `cross_validate` or `cross_val_score`. Store the score of the fold in a variable called `fold_score`. Recall that `cross_validation` in `sklearn` does not shuffle the data by default.    \n",
    "\n",
    "You should start with the following `CountVectorizer` and `LogisticRegression` objects, as well as `X_train` and `y_train` (which you should further split with `train_test_split` and `shuffle=False`):\n",
    "> Note: avoid assigning the variable names `X_train`, `y_train`, `X_test`, `y_test` to your folds when you call `train_test_split`. You'll be using those again later in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "> Meta-comment: you might be wondering why we're going into \"implementation\" here if this course is about _applied_ ML. In CPSC 340, we would go all the way down into `LogisticRegression` and understand how `fit` works, line by line. Here we're not going into that at all, but I still think this type of question (and Exercise 1) is a useful middle ground. I do want you to know what is going on in `Pipeline` and in `cross_validate` even if we don't cover the details of `fit`. To get into logistic regression's `fit` requires a bunch of math; here, we're keeping it more conceptual and avoiding all those prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.6\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fold, X_test_fold, y_fold, y_test_fold = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "countvec.fit(X_fold)\n",
    "X_fold_vectorized = countvec.transform(X_fold)\n",
    "X_test_fold_vectorized = countvec.transform(X_test_fold)\n",
    "\n",
    "lr.fit(X_fold_vectorized, y_fold)\n",
    "predictions = lr.predict(X_test_fold_vectorized)\n",
    "\n",
    "c = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_test_fold.iloc[i]:\n",
    "        c += 1\n",
    "\n",
    "fold_score = c / len(predictions)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9008073817762399"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.6</pre></strong> passed! üåà</p>"
      ],
      "text/plain": [
       "q2.6 results: All test cases passed!"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercise 3: Hyperparameter optimization\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e9e6fdea209d872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.1 Optimizing `max_features` of `CountVectorizer`\n",
    "rubric={points}\n",
    "\n",
    "The following code varies the `max_features` hyperparameter of `CountVectorizer` and makes a plot (with the x-axis on a log scale) that shows train/cross-validation scores vs. `max_features`. It also prints the results. \n",
    "\n",
    "**Your tasks:**\n",
    "- Based on the plot/output, what value of `max_features` seems best? Briefly explain.\n",
    "\n",
    "> The code may take a minute or two to run. You can uncomment the `print` statement if you want to see it show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "max_features = [10, 100, 1000, 10_000, 100_000]\n",
    "\n",
    "for mf in max_features:\n",
    "    #     print(mf)\n",
    "    pipe = make_pipeline(\n",
    "        CountVectorizer(stop_words=\"english\", max_features=mf),\n",
    "        LogisticRegression(max_iter=1000, random_state=123),\n",
    "    )\n",
    "    cv_results = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "    train_scores.append(cv_results[\"train_score\"].mean())\n",
    "    cv_scores.append(cv_results[\"test_score\"].mean())\n",
    "\n",
    "plt.semilogx(max_features, train_scores, label=\"train\")\n",
    "plt.semilogx(max_features, cv_scores, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>train</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.772131</td>\n",
       "      <td>0.771511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.843339</td>\n",
       "      <td>0.839158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.911794</td>\n",
       "      <td>0.884660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.964418</td>\n",
       "      <td>0.895559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.976399</td>\n",
       "      <td>0.895386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_features     train        cv\n",
       "0            10  0.772131  0.771511\n",
       "1           100  0.843339  0.839158\n",
       "2          1000  0.911794  0.884660\n",
       "3         10000  0.964418  0.895559\n",
       "4        100000  0.976399  0.895386"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"max_features\": max_features, \"train\": train_scores, \"cv\": cv_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, 10^3 is the ideal value because the validation accuracy increases sharply up to that point, closely following the training accuracy. However, beyond that point, we start seeing signs of overfitting, as the training accuracy continues to rise significantly while the validation accuracy only shows marginal improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2 Optimizing `C` of `LogisticRegression`\n",
    "rubric={points}\n",
    "\n",
    "The following code varies the `C` hyperparameter of `LogisticRegression` and makes a plot (with the x-axis on a log scale) that shows train/cross-validation scores vs. `C`. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Based on the plot, what value of `C` seems best? Briefly explain. \n",
    "\n",
    "> The code may take a minute or two to run. You can uncomment the `print` statement if you want to see it show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m C \u001b[38;5;129;01min\u001b[39;00m C_vals:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#     print(C)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m make_pipeline(\n\u001b[1;32m      9\u001b[0m         CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m         LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, C\u001b[38;5;241m=\u001b[39mC, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m),\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 12\u001b[0m     cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     train_scores\u001b[38;5;241m.\u001b[39mappend(cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m     15\u001b[0m     cv_scores\u001b[38;5;241m.\u001b[39mappend(cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 423\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m         clone(estimator),\n\u001b[1;32m    426\u001b[0m         X,\n\u001b[1;32m    427\u001b[0m         y,\n\u001b[1;32m    428\u001b[0m         scorer\u001b[39m=\u001b[39;49mscorers,\n\u001b[1;32m    429\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    430\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    431\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    432\u001b[0m         parameters\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    433\u001b[0m         fit_params\u001b[39m=\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mfit,\n\u001b[1;32m    434\u001b[0m         score_params\u001b[39m=\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49mscorer\u001b[39m.\u001b[39;49mscore,\n\u001b[1;32m    435\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    436\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    437\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    438\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m indices\n\u001b[1;32m    441\u001b[0m )\n\u001b[1;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    445\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    890\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    472\u001b[0m         last_step_params \u001b[39m=\u001b[39m routed_params[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 473\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlast_step_params[\u001b[39m\"\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    475\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1350\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[1;32m   1351\u001b[0m     path_func(\n\u001b[1;32m   1352\u001b[0m         X,\n\u001b[1;32m   1353\u001b[0m         y,\n\u001b[1;32m   1354\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[1;32m   1355\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[1;32m   1356\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[1;32m   1357\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   1358\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   1359\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1360\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m   1361\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[1;32m   1362\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1363\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m   1364\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1365\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   1366\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[1;32m   1367\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[1;32m   1368\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[1;32m   1369\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1370\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1372\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[1;32m   1373\u001b[0m )\n\u001b[1;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:455\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    451\u001b[0m l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (C \u001b[39m*\u001b[39m sw_sum)\n\u001b[1;32m    452\u001b[0m iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[1;32m    453\u001b[0m     np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)\n\u001b[1;32m    454\u001b[0m ]\n\u001b[0;32m--> 455\u001b[0m opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[1;32m    456\u001b[0m     func,\n\u001b[1;32m    457\u001b[0m     w0,\n\u001b[1;32m    458\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    459\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    460\u001b[0m     args\u001b[39m=\u001b[39;49m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[1;32m    461\u001b[0m     options\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    462\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_iter,\n\u001b[1;32m    463\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxls\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m50\u001b[39;49m,  \u001b[39m# default is 20\u001b[39;49;00m\n\u001b[1;32m    464\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint,\n\u001b[1;32m    465\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: tol,\n\u001b[1;32m    466\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mftol\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m64\u001b[39;49m \u001b[39m*\u001b[39;49m np\u001b[39m.\u001b[39;49mfinfo(\u001b[39mfloat\u001b[39;49m)\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    467\u001b[0m     },\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    469\u001b[0m n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    470\u001b[0m     solver,\n\u001b[1;32m    471\u001b[0m     opt_res,\n\u001b[1;32m    472\u001b[0m     max_iter,\n\u001b[1;32m    473\u001b[0m     extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    474\u001b[0m )\n\u001b[1;32m    475\u001b[0m w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 713\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    714\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    715\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    716\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    717\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_lbfgsb_py.py:398\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    396\u001b[0m g \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    397\u001b[0m \u001b[39m# x, f, g, wa, iwa, task, csave, lsave, isave, dsave = \\\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m _lbfgsb\u001b[39m.\u001b[39;49msetulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,\n\u001b[1;32m    399\u001b[0m                pgtol, wa, iwa, task, iprint, csave, lsave,\n\u001b[1;32m    400\u001b[0m                isave, dsave, maxls)\n\u001b[1;32m    401\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    403\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "C_vals = 10.0 ** np.arange(-1.5, 2, 0.5)\n",
    "\n",
    "for C in C_vals:\n",
    "    #     print(C)\n",
    "    pipe = make_pipeline(\n",
    "        CountVectorizer(stop_words=\"english\"),\n",
    "        LogisticRegression(max_iter=1000, C=C, random_state=123),\n",
    "    )\n",
    "    cv_results = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "\n",
    "    train_scores.append(cv_results[\"train_score\"].mean())\n",
    "    cv_scores.append(cv_results[\"test_score\"].mean())\n",
    "\n",
    "plt.semilogx(C_vals, train_scores, label=\"train\")\n",
    "plt.semilogx(C_vals, cv_scores, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_scores\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    779\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "pd.DataFrame({\"C\": C_vals, \"train\": train_scores, \"cv\": cv_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal C value on the graph is at the midway point between 10^-1 and 1, where the accuracy peaks. Beyond this point, the model starts overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.3 Hyperparameter optimization \n",
    "rubric={autograde}\n",
    "\n",
    "Start with the pipeline `pipe` below.\n",
    "\n",
    "**Your tasks:**\n",
    "- Create a `GridSearchCV` object named `grid_search` to jointly optimize `max_features` of `CountVectorizer` and `C` of `LogisticRegression` across all the combinations of values we tried above. \n",
    "- What are the best values of `max_features` and `C` according to your grid search? Store them in variables `best_max_features` and `best_C`, respectively.  \n",
    "- Store the best score returned by the grid search in a variable called `best_score`. \n",
    "\n",
    "> The code might be a bit slow here. Setting `n_jobs=-1` should speed it up if you have a multi-core processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    CountVectorizer(stop_words=\"english\"),\n",
    "    LogisticRegression(max_iter=1000, random_state=123),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_param = {\n",
    "    'countvectorizer__max_features': max_features,\n",
    "    'logisticregression__C': C_vals\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe_lr, grid_param, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_max_features = grid_search.best_params_['countvectorizer__max_features']\n",
    "best_C = grid_search.best_params_['logisticregression__C']\n",
    "best_score = grid_search.best_score_\n",
    "print(best_C)\n",
    "print(best_max_features)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3.3</pre></strong> passed! üöÄ</p>"
      ],
      "text/plain": [
       "q3.3 results: All test cases passed!"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.4 Discussion \n",
    "rubric={points}\n",
    "\n",
    "- Do the best values of hyperparameters found by Grid Search agree with what you found in 3.1 and 3.2? \n",
    "- Generally speaking, _should_ these values agree with what you found in parts  3.1 and 3.2? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They mostly align with the values I found, but my chosen value for C is slightly off. Generally, it's not essential for them to match exactly, as my intuition might differ from the algorithm's approach. For example, I might feel that the marginal gains in validation accuracy don't justify the overfitting, whereas the algorithm could see it differently, leading to slightly different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.5 Test score\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "- Evaluate your final model on the test set. Store the test accuracy in the variable called `test_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_l = make_pipeline(\n",
    "    CountVectorizer(stop_words=\"english\", max_features=10000),\n",
    "    LogisticRegression(max_iter=1000, random_state=123, C=1.0)\n",
    ")\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "test_score = pipe_lr.score(X_test, y_test)\n",
    "\n",
    "test_score\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3.5</pre></strong> passed! üôå</p>"
      ],
      "text/plain": [
       "q3.5 results: All test cases passed!"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.6 Discussion\n",
    "rubric={points}\n",
    "\n",
    "- How does your test accuracy compare to your validation accuracy? \n",
    "- If they are different: do you think this is because you \"overfitted on the validation set\", or simply random luck?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.6\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values seem very close to me, which suggests that we likely selected the right hyperparameters and optimized them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 4: Very short answer questions\n",
    "<hr>\n",
    "rubric={points}\n",
    "\n",
    "Each question is worth 2 points. Max 2 sentences per answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. What is the problem with calling `fit_transform` on your test data with `CountVectorizer`? \n",
    "2. If you could only access one of `predict` or `predict_proba`, which one would you choose? Briefly explain.\n",
    "3. What are two advantages of `RandomizedSearchCV` over `GridSearchCV`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This violates the golden rule because it uses the test data to fit the count vectorizer, which allows the test data to influence the model‚Äôs results. This leads to a breach of the golden rule of keeping test data completely separate from the training process.\n",
    "\n",
    "2. I would choose predict_proba because it provides the probabilities of the predictions, and knowing the threshold allows us to derive the actual predictions by default. Therefore, predict_proba offers more information, including the model's confidence, making it a better option.\n",
    "\n",
    "3. It is much faster because it doesn't need to evaluate every single value, which reduces the computing power required. This efficiency also allows us to cover a much broader range of values, increasing the likelihood that the optimal value will fall within our range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Congratulations on finishing the homework! \n",
    "\n",
    "![](./img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_train_dummy_tests = np.random.randn(51, 5)\n>>> y_train_dummy_tests = np.append(np.ones(26), np.zeros(25))\n>>> X_valid_dummy_tests = np.random.randn(11, 5)\n>>> y_valid_dummy_tests = np.append(np.zeros(7), np.ones(4))\n>>> test_dc = MyDummyClassifier()\n>>> test_dc.fit(X_train_dummy_tests, y_train_dummy_tests)\n>>> assert np.array_equal(test_dc.predict(X_train_dummy_tests), np.ones(51)), 'Tests for predict failed'\n>>> assert np.array_equal(test_dc.predict(X_valid_dummy_tests), np.ones(11)), 'Tests for predict failed'\n>>> predict_test = test_dc.predict_proba(X_train_dummy_tests)\n>>> assert np.all(predict_test == predict_test[0]), 'Tests for predict_proba failed'\n>>> assert np.allclose(predict_test[0], np.mean(test_dc.predict_proba(X_valid_dummy_tests), axis=0), [0.49, 0.51]), 'Tests for predict_proba failed'\n>>> assert np.isclose(test_dc.score(X_train_dummy_tests, y_train_dummy_tests), 0.51, atol=0.01), 'Tests for score failed'\n>>> assert np.isclose(test_dc.score(X_valid_dummy_tests, y_valid_dummy_tests), 0.364, atol=0.01), 'Tests for score failed'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_train_dummy_tests = np.random.randn(35, 5)\n>>> y_train_dummy_tests = np.append(np.ones(10), np.zeros(25))\n>>> X_valid_dummy_tests = np.random.randn(7, 5)\n>>> y_valid_dummy_tests = np.append(np.zeros(1), np.ones(6))\n>>> test_dc = MyDummyClassifier()\n>>> test_dc.fit(X_train_dummy_tests, y_train_dummy_tests)\n>>> assert np.all(test_dc.predict(X_train_dummy_tests) == 0), 'Tests for predict failed'\n>>> assert np.all(test_dc.predict(X_valid_dummy_tests) == 0), 'Tests for predict failed'\n>>> predict_test = test_dc.predict_proba(X_train_dummy_tests)\n>>> assert np.all(predict_test == predict_test[0]), 'Tests for predict_proba failed'\n>>> assert np.allclose(predict_test[0], np.mean(test_dc.predict_proba(X_valid_dummy_tests), axis=0), [0.714, 0.286]), 'Tests for predict_proba failed'\n>>> assert np.isclose(test_dc.score(X_train_dummy_tests, y_train_dummy_tests), 0.714, atol=0.01), 'Tests for score failed'\n>>> assert np.isclose(test_dc.score(X_valid_dummy_tests, y_valid_dummy_tests), 0.143, atol=0.01), 'Tests for score failed'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not dummy_cv_score is None, 'Are you using the provided variable?'\n>>> assert sha1(str(np.round(dummy_cv_score, 3)).encode('utf-8')).hexdigest() == '838befe9ffa0a0d530805ba36c2e4890d4148ba1', \"Your mean CV score doesn't look correct.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.4": {
     "name": "q2.4",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not tweet is None, 'Are you using the correct variable to store the tweet?'\n>>> assert not prob is None, 'Are you using the correct variable to store the probability?'\n>>> assert sha1(str(np.round(prob, 4)).encode('utf-8')).hexdigest() == 'e8dc057d3346e56aed7cf252185dbe1fa6454411', 'Incorrect probability.'\n>>> assert sha1(str(tweet).encode('utf8')).hexdigest() == 'fe39d5cbae2b335b4fde5486a8df189e41392043', 'Incorrect tweet text.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.5": {
     "name": "q2.5",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not top_5_words is None, 'Are you using the correct variable?'\n>>> assert not bottom_5_words is None, 'Are you using the correct variable?'\n>>> assert len(top_5_words) == 5, 'Are you getting the top 5 words?'\n>>> assert len(bottom_5_words) == 5, 'Are you getting the bottom 5 words?'\n>>> assert isinstance(top_5_words, list), 'Are you storing the top 5 words in a list?'\n>>> assert isinstance(bottom_5_words, list), 'Are you storing the bottom 5 words in a list?'\n>>> assert my_sha1(''.join(sorted(top_5_words))) == 'd57c4319aa6e91c93c59a8c2ed01401cbad7e072', 'Incorrect words in `top_5_words`.'\n>>> assert my_sha1(''.join(sorted(bottom_5_words))) == '5e2af1c788307cca183334510cf6bbee0208cab2', 'Incorrect words in `bottom_5_words`.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.6": {
     "name": "q2.6",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not fold_score is None, 'Are you using the correct variable?'\n>>> assert my_sha1(str(np.round(fold_score, 3))) == 'bd9fa29183f2f3b2e0484dc70993b5d40591f326', \"The fold_score doesn't look right.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.3": {
     "name": "q3.3",
     "points": [
      5,
      1,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not best_max_features is None, 'Are you using the correct variable?'\n>>> assert not best_C is None, 'Are you using the correct variable?'\n>>> assert X_train.shape == (17340,), 'X_train is the wrong shape. Did you overwrite it in q2.6?'\n>>> assert y_train.shape == (17340,), 'y_train is the wrong shape. Did you overwrite it in q2.6?'\n>>> assert type(grid_search.get_params()['estimator']) == Pipeline, 'Are you passing a pipeline to the GridSearch?'\n>>> assert len(grid_search.get_params()['param_grid']['countvectorizer__max_features']) == 5, 'Are you using the max_features values from 3.1?'\n>>> assert len(grid_search.get_params()['param_grid']['logisticregression__C']) == 7, 'Are you using the C values from 3.2?'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> possible_max_feats = ['8a12a315082a345f1a9d3ad14b214cd36d310cf8', '409e9519c66216726447bd4a07d6aed0475338cc']\n>>> assert my_sha1(str(best_max_features)) in possible_max_feats, \"Best max feature doesn't seem correct\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert my_sha1(str(best_C)) == 'e8dc057d3346e56aed7cf252185dbe1fa6454411', \"Best C doesn't seem correct.\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert my_sha1(str(np.round(best_score, 2))) == '1469842b4307d36cccb487dc989f21016daadbcc', \"Best score doesn't seem correct.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.5": {
     "name": "q3.5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not test_score is None, 'Are you storing the score in the provided variable?'\n>>> assert X_test.shape == (26012,), 'X_test is the wrong shape. Did you overwrite it in q2.6?'\n>>> assert y_test.shape == (26012,), 'y_test is the wrong shape. Did you overwrite it in q2.6?'\n>>> assert my_sha1(str(np.round(test_score, 2))) == '86cc84573855498bf3bf582affc07c1fecfc661c', \"The test score doesn't look correct.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
